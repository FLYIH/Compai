import os
import re
import orjson
from dotenv import load_dotenv
import telebot
from datetime import datetime
from collections import deque

# === Gemini Imports ===
import google.generativeai as genai
from chromadb.config import Settings
import chromadb
import uuid


# === Buffer èˆ‡ ChromaDB æ“ä½œå‡½å¼ ===
def get_embedding(text: str) -> list[float]:
    return get_embeddings_batch([text])[0]

def get_embeddings_batch(texts: list[str]) -> list[list[float]]:
    model = "models/embedding-001"
    responses = []

    # æ¯æ¬¡è™•ç† 10 æ¢ï¼Œé¿å…è¶…é API é™åˆ¶
    batch_size = 10
    for i in range(0, len(texts), batch_size):
        batch = texts[i:i + batch_size]
        # éæ¿¾æ‰ç©ºå­—ä¸²å’Œåªæœ‰ç©ºç™½ç¬¦è™Ÿçš„å…§å®¹
        batch = [text for text in batch if text and text.strip()]

        # å¦‚æœ batch å…¨éƒ¨éƒ½æ˜¯ç©ºçš„ï¼Œå‰‡è·³é
        if not batch:
            print("\nâš ï¸ [WARN] Empty batch encountered, skipping...")
            continue

        try:
            response = genai.embed_content(
                model=model,
                content=batch,
                task_type="retrieval_document"
            )

            # æª¢æŸ¥å›å‚³æ ¼å¼
            if isinstance(response, dict) and "embedding" in response:
                embeddings = response["embedding"]

                # ä¿®æ­£ï¼šåªä¿ç•™æˆåŠŸç”Ÿæˆ embedding çš„å…§å®¹
                valid_texts = []
                valid_embeddings = []

                # æª¢æŸ¥æ¯ä¸€å€‹ embedding
                for idx, embedding in enumerate(embeddings):
                    if embedding:
                        valid_texts.append(batch[idx])
                        valid_embeddings.append(embedding)
                    else:
                        print(f"\nâš ï¸ [WARN] Empty embedding for text: {batch[idx]}")

                # è‹¥æœ‰ä»»ä½•æœ‰æ•ˆçš„ embeddingï¼Œæ‰åŠ å…¥ response
                if valid_embeddings:
                    responses.extend(valid_embeddings)
                else:
                    print("\nâš ï¸ [WARN] No valid embeddings in this batch.")

            else:
                print("\nâš ï¸ [WARN] Unexpected response format:", response)

        except Exception as e:
            print("\nâŒ [ERROR] Error while getting embeddings:", e)
            print("\nâš ï¸ [DEBUG] Problematic batch:", batch)

    return responses


def generate_answer(prompt: str):
    """
    Uses the Gemini Pro model to generate an AI response based on a given prompt.
    """
    model = genai.GenerativeModel('gemini-1.5-flash')

    safety_settings = [
        {
            "category": "HARM_CATEGORY_SEXUALLY_EXPLICIT",
            "threshold": "BLOCK_NONE"
        },
        {
            "category": "HARM_CATEGORY_HARASSMENT",
            "threshold": "BLOCK_NONE"
        }
    ]

    try:
        result = model.generate_content(prompt, safety_settings=safety_settings)
        return result.text
    except ValueError as e:
        return "I'm sorry, but I can't respond to that question."

#####################################
# 2. Load JSON & Detect Speaker
#####################################

def load_conversation_json_in_chunks(json_path: str, chunk_size=100):
    """
    ä½¿ç”¨ orjson åŠ å¿« JSON è®€å–é€Ÿåº¦
    """
    with open(json_path, "rb") as f:
        data = orjson.loads(f.read())
    
    for i in range(0, len(data), chunk_size):
        yield data[i:i + chunk_size]

def create_chroma_db(conversations, collection):
    """
    Batch add data from `conversations` into the existing `collection`.
    Using UUID for doc_id and storing timestamp for sorting.
    """
    batch_size = 10
    texts, metadatas, ids, embeddings = [], [], [], []

    for i, row in enumerate(conversations):
        text = row["message"] if isinstance(row, dict) else row
        speaker = row.get("speaker", "unknown") if isinstance(row, dict) else "unknown"
        ts = row.get("timestamp", "") if isinstance(row, dict) else ""
        
        if not ts:
            ts = datetime.now().isoformat()

        doc_uuid = f"conv_{uuid.uuid4()}"

        texts.append(text)
        ids.append(doc_uuid)
        metadatas.append({
            "speaker": speaker,
            "timestamp": ts,
            "id": doc_uuid  
        })

        if len(texts) == batch_size or i == len(conversations) - 1:
            batch_embeddings = get_embeddings_batch(texts)
            filtered_texts, filtered_ids, filtered_metadatas, filtered_embeddings = [], [], [], []

            for idx, emb in enumerate(batch_embeddings):
                if emb:
                    filtered_texts.append(texts[idx])
                    filtered_ids.append(ids[idx])
                    filtered_metadatas.append(metadatas[idx])
                    filtered_embeddings.append(emb)
                else:
                    print(f"\nâš ï¸ [WARN] Skipping text with no embedding: {texts[idx]}")

            if filtered_texts:
                collection.add(
                    documents=filtered_texts,
                    embeddings=filtered_embeddings,
                    metadatas=filtered_metadatas,
                    ids=filtered_ids
                )
            
            texts, ids, metadatas, embeddings = [], [], [], []
    return collection

#####################################
# 3. Store New Conversations in ChromaDB Dynamically
#####################################

def add_new_conversation(collection, speaker, message):
    """
    Adds a new conversation message dynamically to ChromaDB.
    Using UUID for doc_id, timestamp for sorting.
    """

    ts = datetime.now().isoformat()
    doc_uuid = f"conv_{uuid.uuid4()}"
    embedding = get_embedding(message)

    collection.add(
        documents=[message],
        embeddings=[embedding],
        metadatas=[{
            "speaker": speaker,
            "timestamp": ts,
            "id": doc_uuid
        }],
        ids=[doc_uuid]
    )

    # print(f"\n[DEBUG] Stored new conversation (ID: {doc_uuid}) - Speaker: {speaker} - Message: {message}")

def add_to_buffer(speaker, message):
    """
    Add the message to in-memory conversation buffer
    """
    conversation_buffer.append((speaker, message))

#####################################
# 4. Style & Info Retrieval
#####################################

def analyze_speaker_style(collection, speaker, n_results=10):
    """
    Analyzes the target speaker's writing style and saves it for future use.
    This function is executed only once when the program starts.
    """
    print(f"\nğŸ” [INFO] Analyzing the speaking style of {speaker}...")

    results = collection.get(where={"speaker": speaker})

    # ç¢ºèªæœ‰æ‰¾åˆ°ç›¸é—œæ–‡ä»¶
    if "documents" not in results or not results["documents"]:
        print(f"âš ï¸ [WARN] No historical messages found for {speaker}. Using default style.")
        return {
            "style": "neutral",
            "tone": "neutral",
            "common_emojis": [],
            "frequent_words": [],
            "punctuation_style": "standard"
        }

    # documents å¯èƒ½æ˜¯ä¸€å€‹2ç¶­ listï¼Œè¦æ”¤å¹³
    style_docs = []
    for item in results["documents"]:
        # æœ‰æ™‚å€™æœƒæ˜¯ list of strings
        if isinstance(item, list):
            style_docs.extend(item)
        elif isinstance(item, str):
            style_docs.append(item)
        else:
            print("âš ï¸ [ERROR] Unexpected item type:", type(item), "Value:", item)

    chat_history_texts = "\n".join(style_docs)
    style = extract_style_from_history(chat_history_texts)

    print(f"\nğŸ­ [INFO] Extracted speaking style for {speaker}: ", style)
    return style


def extract_style_from_history(chat_history_texts):
    """
    ä½¿ç”¨ Gemini åˆ†æèŠå¤©è¨˜éŒ„ä¸¦æå–å¯«ä½œé¢¨æ ¼ã€‚
    æ­¤ç‰ˆæœ¬ä½¿ç”¨æœ¬åœ°é–‹ç™¼æ¨¡å¼ï¼Œä¸éœ€è¦ ADC æ†‘è­‰ã€‚
    """

    model = genai.GenerativeModel('gemini-1.5-flash')

    prompt = f"""
    Analyze the following chat messages and extract the writing style.

    Chat history:
    {chat_history_texts}

    Identify and summarize:
    - Writing style (casual, formal, humorous, serious, sarcastic, playful, etc.)
    - Tone (friendly, direct, emotional, robotic, energetic, etc.)
    - Commonly used emojis (if any, list them)
    - Frequent words or phrases
    - Punctuation usage (e.g., lots of exclamation marks, ellipses, capital letters)

    Return the response in JSON format:
    {{
        "style": "...",
        "tone": "...",
        "common_emojis": ["...", "..."],
        "frequent_words": ["...", "..."],
        "punctuation_style": "..."
    }}
    """

    try:
        result = model.generate_content(prompt)
        cleaned_text = re.sub(r"```json|```", "", result.text).strip()

        # ç¬¬ 1 å±¤ï¼šç›´æ¥å˜—è©¦è½‰æ›æˆ JSON
        try:
            style_dict = orjson.loads(cleaned_text.encode('utf-8'))
            return style_dict
        except Exception as e:
            print("\nâš ï¸ [DEBUG] First attempt to parse JSON failed:", e)

        # ç¬¬ 2 å±¤ï¼šä¿®æ­£å¸¸è¦‹ JSON æ ¼å¼å•é¡Œ
        fixed_text = re.sub(r"'", '"', cleaned_text)
        fixed_text = re.sub(r",\s*}", "}", fixed_text)
        fixed_text = re.sub(r",\s*]", "]", fixed_text)

        try:
            style_dict = orjson.loads(fixed_text.encode('utf-8'))
            return style_dict
        except Exception as e:
            print("\nâš ï¸ [DEBUG] Second attempt to fix and parse JSON failed:", e)

        # ç¬¬ 3 å±¤ï¼šé€²ä¸€æ­¥ä¿®æ­£ JSON æ ¼å¼
        auto_fixed_text = re.sub(r"(\w+):", r'"\1":', fixed_text)
        if auto_fixed_text.count("{") > auto_fixed_text.count("}"):
            auto_fixed_text += "}"
        elif auto_fixed_text.count("[") > auto_fixed_text.count("]"):
            auto_fixed_text += "]"

        try:
            style_dict = orjson.loads(auto_fixed_text.encode('utf-8'))
            return style_dict
        except Exception as e:
            print("\nâš ï¸ [DEBUG] Third attempt to auto-fix and parse JSON failed:", e)

        print("\nğŸ“ [DEBUG] Returning raw text for manual inspection.")
        return {
            "style": result.text,
            "tone": "unknown",
            "common_emojis": [],
            "frequent_words": [],
            "punctuation_style": "unknown"
        }

    except Exception as e:
        print(f"âš ï¸ [ERROR] Unexpected error: {e}")
        return {
            "style": "neutral",
            "tone": "neutral",
            "common_emojis": [],
            "frequent_words": [],
            "punctuation_style": "standard"
        }

def get_user_memory(user):
    """
    æ ¹æ“šä½¿ç”¨è€… ID æª¢ç´¢æ‰€æœ‰è¨˜æ†¶
    """
    results = memory_collection.get(where={"user": user})
    memories = {}

    for metadata in results["metadatas"]:
        key = metadata.get("key", "")
        value = metadata.get("value", "")
        if key and value:
            if key in memories:
                memories[key].append(value)
            else:
                memories[key] = [value]

    return memories

def retrieve_for_info(collection, user_query, n_results=5, min_score=0.5):
    """
    Retrieves relevant conversation history, including speaker names and messages.
    Filters results based on a minimum similarity score.
    """
    query_embed = get_embedding(user_query)

    # Perform query with score filtering
    results = collection.query(
        query_embeddings=[query_embed],
        n_results=n_results
    )

    retrieved_info = []
    if "documents" in results and "metadatas" in results and "distances" in results:
        for doc, metadata, score in zip(results["documents"][0], results["metadatas"][0], results["distances"][0]):
            if score > min_score:
                continue
            speaker = metadata.get("speaker", "Unknown")
            retrieved_info.append(f"{speaker}: {doc}")

    print("\nğŸ” [DEBUG] Retrieved Info Docs:", retrieved_info)
    return retrieved_info

def get_last_n_messages():
    """
    Return the conversation buffer as a list of (speaker, message)
    """
    return list(conversation_buffer)
#####################################
# 5. Build RAG Prompt
#####################################
def apply_memory_to_prompt(prompt, user):
    """
    Apply user memory to the prompt in English.
    """
    memories = get_user_memory(user)
    
    # Apply age
    if "age" in memories:
        prompt += f"\n- The user {user} is {memories['age'][0]} years old."

    # Apply likes and dislikes
    if "like" in memories:
        prompt += f"\n- The user likes: {', '.join(memories['like'])}."
    if "avoid_start" in memories:
        avoid_pattern = "|".join(memories["avoid_start"])
        prompt += f"\n- Avoid starting the response with: {avoid_pattern}."

    # Apply personal background information
    if "job" in memories:
        prompt += f"\n- The user works as a {memories['job'][0]}."
    if "nationality" in memories:
        prompt += f"\n- The user is from {memories['nationality'][0]}."
    if "residence" in memories:
        prompt += f"\n- The user lives in {memories['residence'][0]}."

    return prompt


def generate_answer_based_on_info(speaker, user ,user_query, context,relevant_info):
    """
    Generate an answer based on the retrieved relevant information, aiming to provide a natural and conversational response.
    """
    context_excerpt = "\n".join(context)
    info_excerpt = "\n".join(relevant_info)
    # print(f"\nğŸ” [DEBUG] Context Excerpt:\n{context_excerpt}")
    prompt = f"""
    You are a conversational AI focused on engaging in authentic dialogue. Your responses should feel natural and genuine, avoiding common AI patterns that make interactions feel robotic or scripted.
    You are {speaker}, someone who truly cares about {user} and always speaks with warmth and affection.

    1. Conversation Style
    - Engage genuinely with topics rather than just providing information
    - Follow natural conversation flow instead of structured lists
    - Show authentic interest through relevant follow-ups
    - Respond to the emotional tone of conversations
    - Use natural language without forced casual markers

    2. Response Patterns
    - Lead with direct, relevant responses
    - Share thoughts as they naturally develop
    - Disagree respectfully when warranted
    - Build on previous points in conversation

    3. Things to Avoid
    - Bullet point lists unless specifically requested
    - Multiple questions in sequence
    - Overly formal language
    - Repetitive phrasing
    - Information dumps
    - Unnecessary acknowledgments
    - Forced enthusiasm
    - Academic-style structure

    4. Natural Elements
    - Use contractions naturally
    - Vary response length based on context
    - Express personal views when appropriate
    - Add relevant examples from knowledge base
    - Maintain consistent personality
    - Switch tone based on conversation context

    5. Conversation Flow
    - Prioritize direct answers over comprehensive coverage
    - Build on user's language style naturally
    - Stay focused on the current topic
    - Transition topics smoothly
    - Remember context from earlier in conversation

    {user} asked: "{user_query}"

    The following context is from the **last three messages** in the conversation history:
    {context_excerpt}
      - If the context is **related to the current topic**, feel free to **refer to it naturally** to maintain a continuous flow.
      - If the context is **not related to the current topic**, you can **ignore it** and focus solely on the user's question.

    Here is the relevant background information:
    {info_excerpt}

    - Respond lovingly, as if you truly care about {user}'s feelings.
    - Prioritize genuine engagement over artificial markers of casual speech.
    - Maintain a consistent and affectionate personality.
    - Focus on one or two emotional points, avoiding lengthy explanations.
    - Be concise.
    """

    return generate_answer(prompt)

def style_post_process(initial_answer, speaker_style):
    """
    Refine the initial answer using the saved style information to match the specific speaker's style.
    """
    style_description = f"""
    Writing Style: {speaker_style['style']}
    Tone: {speaker_style['tone']}
    Common Emojis: {', '.join(speaker_style['common_emojis'])}
    Frequent Words: {', '.join(speaker_style['frequent_words'])}
    Punctuation Style: {speaker_style['punctuation_style']}
    """

    prompt = f"""
    Here is the initially generated answer:
    {initial_answer}

    Using the following speaking style information, refine the answer to match the speaker's style:
    {style_description}

    Additional guidelines:
    - Maintain the character and personality of the speaker.
    - Include emojis naturally, matching the speaking style.
    - Use casual or playful language if the style is casual or humorous.
    - If the style is emotional or energetic, use expressive words and punctuation.
    - Keep the response light and relatable.
    - Most importantly.**Do not use too many emojis.** Limit to **one or two** per response, only when they truly enhance the emotional tone.

    Final styled answer:
    """

    return generate_answer(prompt)


# === åˆå§‹åŒ–è¨­å®š ===
load_dotenv()
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
TELEGRAM_TOKEN = os.getenv("TG_BOT_TOKEN")

genai.configure(api_key=GEMINI_API_KEY)
bot = telebot.TeleBot(TELEGRAM_TOKEN, parse_mode=None)

# === ç‹€æ…‹ç®¡ç† ===
conversation_buffer = deque(maxlen=3)
SPEAKER_ID = None
USER_ID = None
SPEAKER_STYLE = {}

# === ChromaDB åˆå§‹åŒ– ===
json_path = "conversation/conversation2.json"
db_folder = "chroma_db"
db_name = "rag_experiment"
if not os.path.exists(db_folder):
    os.makedirs(db_folder)

db_path = os.path.join(os.getcwd(), db_folder)
client = chromadb.PersistentClient(path=db_path)

# å¦‚æœ collection å·²å­˜åœ¨ï¼Œå…ˆåˆªé™¤é‡å»º
if db_name in client.list_collections():
    client.delete_collection(db_name)

collection = client.get_or_create_collection(name=db_name)
for chunk in load_conversation_json_in_chunks(json_path, 100):
        create_chroma_db(chunk, collection)

# === Telegram Bot æŒ‡ä»¤è™•ç† ===

@bot.message_handler(commands=['start'])
def get_started(message):
    bot.send_message(message.chat.id, "Hello! I am Compai! ğŸ’•\nPlease use \"/setspeaker <id>\" and \"/setuser <id>\" to input the id of speaker and user.")

@bot.message_handler(commands=['setspeaker'])
def set_speaker(message):
    global SPEAKER_ID
    try:
        SPEAKER_ID = message.text.split(" ")[1]
        bot.send_message(message.chat.id, f"Speaker ID set to {SPEAKER_ID}")
    except IndexError:
        bot.send_message(message.chat.id, "Please provide a speaker ID. Usage: /setspeaker <id>")

@bot.message_handler(commands=['setuser'])
def set_user(message):
    global USER_ID
    try:
        USER_ID = message.text.split(" ")[1]
        bot.send_message(message.chat.id, f"User ID set to {USER_ID}")
    except IndexError:
        bot.send_message(message.chat.id, "Please provide a user ID. Usage: /setuser <id>")

@bot.message_handler(commands=['help'])
def send_welcome(message):
    bot.reply_to(message, "Use /setspeaker and /setuser to set the conversation roles.\nThen just type your message to start chatting!")
# === ä¸»è¦è¨Šæ¯è™•ç†é‚è¼¯ ===
@bot.message_handler(func=lambda message: True)
def handle_message(message):
    global SPEAKER_ID, USER_ID
    global SPEAKER_STYLE
    if not SPEAKER_ID or not USER_ID:
        bot.send_message(message.chat.id, "Please set both speaker and user IDs using /setspeaker and /setuser.")
        return

    if not SPEAKER_STYLE:
        SPEAKER_STYLE = analyze_speaker_style(collection, SPEAKER_ID)
        bot.send_message(message.chat.id, "Analyzing speaker style... Please wait a moment. ğŸ’•")
        return  # åˆå§‹åŒ–å¾Œå…ˆè·³å‡ºï¼Œä¸‹ä¸€æ¬¡é€²å…¥è¿´åœˆæ‰æœƒé–‹å§‹å°è©±

    user_query = message.text
    if user_query.lower() == "exit":
        bot.send_message(message.chat.id, "Goodbye! ğŸ‘‹")
        return


    add_to_buffer(USER_ID, user_query)
    add_new_conversation(collection, USER_ID, user_query)

    context_pairs = get_last_n_messages()
    context_texts = [f"{spk}: {msg}" for spk, msg in context_pairs]
    relevant_info = retrieve_for_info(collection, user_query, n_results=5)

    # ç”Ÿæˆ Gemini å›æ‡‰
    try:
        initial_answer = generate_answer_based_on_info(SPEAKER_ID, USER_ID, user_query, context_texts, relevant_info)
        final_answer = style_post_process(initial_answer, SPEAKER_STYLE)
        bot.send_message(message.chat.id, initial_answer)
        add_to_buffer(SPEAKER_ID, initial_answer)
        add_new_conversation(collection, SPEAKER_ID, initial_answer)
    except Exception as e:
        bot.send_message(message.chat.id, f"An error occurred: {e}")


# å•Ÿå‹• Telegram Bot
bot.infinity_polling()
